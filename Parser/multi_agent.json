{
    "page_1": [
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Arabian Journal for Science and Engineering"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "https://doi.org/10.1007/s13369-017-3018-9"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 9,
                "text": "R E S E A R C H A R T I C L E - S Y S T E M S E N G I N E E R I N G"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 16,
                "text": "Fuzzy Q-Learning-Based Multi-agent System for Intelligent Traf\ufb01c"
            },
            {
                "bold": true,
                "font_size": 16,
                "text": "Control by a Game Theory Approach"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Abolghasem Daeichian1 \u00b7 Amir Haghani2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Received: 11 December 2015 / Accepted: 3 December 2017"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "\u00a9 King Fahd University of Petroleum & Minerals 2017"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 10,
                "text": "Abstract"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "This paper introduces a multi-agent approach to adjust traf\ufb01c lights based on traf\ufb01c situation in order to reduce average delay"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "time. In the traf\ufb01c model, lights of each intersection are controlled by an autonomous agent. Since decision of each agent"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "affects neighbor agents, this approach creates a classical non-stationary environment. Thus, each agent not only needs to"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "learn from the past experience but also has to consider decision of neighbors to overcome dynamic changes of the traf\ufb01c"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "network. Fuzzy Q-learning and Game theory are employed to make policy based on previous experiences and decision of"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "neighbor agents. Simulation results illustrate the advantage of the proposed method over \ufb01xed time, fuzzy, Q-learning and"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "fuzzy Q-learning control methods."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "Keywords Traf\ufb01c control \u00b7 Multi-agent system \u00b7 Game theory \u00b7 Fuzzy Q-learning"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 12,
                "text": "1 Introduction"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "Urbanization, increasing number of vehicles, and lack of"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "transport infrastructures have increased travel time, fuel con-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "sumption, and air pollution. Therefore, urban life equals with"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "waste of time, less clean air, and acoustic pollution. Con-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ventional \ufb01xed traf\ufb01c management systems are not able to"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "\ufb01ght complexity and dynamic of large traf\ufb01c networks. While"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "arti\ufb01cial intelligence (AI) are greatly employed to develop"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "intelligent traf\ufb01c systems (ITS) [6,7,19,24], multi-agent sys-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "tem is an approach to model ITS [25,30]. This framework"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "consists of a population of intelligent and autonomous agents"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "work together in an environment [27]. Traf\ufb01c lights [20],"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "vehicles [3], and pedestrians [29] are considered as agents"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "in modeling of urban traf\ufb01c networks. Each agent needs"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "to learn from the past experiences which is a key point to"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "approximate a better decision-making policy. Multi-agent"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "model-based [32] as well as model-free [12] reinforcement"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "learning (RL) techniques are widely used in researches on"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ITS [6,23]."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "B Abolghasem Daeichian"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "a-daeichian@araku.ac.ir; a.daeichian@gmail.com"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1 Department of Electrical Engineering, Faculty of"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Engineering, Arak University, Arak 38156-8-8349, Iran"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "2 Department of Electrical Engineering, Payam Institute of"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Higher Education, Golpayegan, Isfahan, Iran"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "In a multitude of researches, any agent only considers its"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "own traf\ufb01c state in order to determine the control policy."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "For example, single intersection with two phases is investi-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "gated in [2]. Length of vehicles queue waiting on the light"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "is considered as state which can be measured by the agent. It"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "decides on extend green time or change it to the next phase so"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "that the number of vehicles waiting on the light is minimized."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "The results show superiority of Q-learning agent over uni-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "form traf\ufb01c \ufb02ows and constant-ratio traf\ufb01c \ufb02ows. In [32],"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "traf\ufb01c lights are considered as agents which communicate"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "with vehicles. The vehicles estimate their mean waiting time"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "and transmit this time to traf\ufb01c light where a popular RL"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "algorithm, namely Q-learning, is used to provide a control"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "for traf\ufb01c signal scheduling. Results of this study show 22%"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "reduction in waiting time compared to constant time lights."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Multi-objective reinforcement learning is utilized to control"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "several traf\ufb01c lights in [17]. Optimization goals include num-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ber of stops of a vehicle, mean stopping time, and length of"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "vehicles\u2019 queue on the next intersection. Its results indicate"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "that multi-RL can effectively prevent the queue spillovers"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "under congested condition to avoid large-scale traf\ufb01c jams."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Bull et al. [10] used learner classi\ufb01ers to control light traf-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "\ufb01c including 4 intersections. In this research, traf\ufb01c lights"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "include two phases at each intersection, where one phase is"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "for moving north\u2013south and one is for east-west. Controller"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "at each intersection obtains optimum phase time through"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "extracting if-then rules. Its results show that performance of"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 15,
                "text": "123"
            }
        ]
    ],
    "page_2": [
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "the traf\ufb01c light using learner classi\ufb01er system has improved"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "signi\ufb01cantly compared to constant time traf\ufb01c light. In [28],"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the learning purpose is modeled in such a way that states"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "indications are based on the summation of the cars wait-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ing times. Obviously, the more cars information is received,"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the model will be more complicated and state space will be"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "larger. This issue is one of the signi\ufb01cant problems of large"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "networks. Adaptive control, which is introduced in [23],"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "uses the approximate of a function as mapping of states to"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "scheduling. Fuzzy inference engine is exploited to decrease"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "systematic faults of Q-algorithm in [22]. The results demon-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "strate that not only learning in fuzzy framework is done faster"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "than Q-learning but also delay in intersections is decreased"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "considerably. A multi-agent fuzzy approach is proposed in"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "[18], where Q-learning updates the set of rule base in fussy"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "inference engine. In [13], a new method which has the capa-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "bility to estimate an incomplete model of environment is"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "described for a given non-static environment. This method"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "is applied in a network composed of 9 intersections. The"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "reported results show that this method has better performance"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "than the model-free methods and model-based methods, but"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "could not be generalized and used in larger networks."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "In other researches, agents consider other agents in"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "determination of their own control policy. For instance, coor-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "dination among agents is desired in [21] where the agents not"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "only consider number of waiting vehicles on its own inter-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "section, but also they consider number of vehicles which"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "have stopped in adjacent intersections. The RL is applied"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "on 5 intersections within three different scenario. The over-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "all results show improvement in delay time. In [32], RL is"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "used to control the traf\ufb01c in a grid where a type of cooper-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ative learning simultaneously controls the traf\ufb01c signals and"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "determines the optimal routes. One of the main drawbacks"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "of this method is the high costs of communication and infor-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "mation exchange, speci\ufb01cally when intersections of network"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "are increased. Cooperative RL tries to extract the knowledge"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "from neighbor agents in a scheduling learning [26]. This"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "method is implemented in an area of Dublin including 64"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "intersections."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "This paper introduces a hybrid fuzzy Q-learning and Game"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "theory method for control of traf\ufb01c lights in multi-agent"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "framework. It exploits the bene\ufb01ts of fuzzi\ufb01cation as well as"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "interaction with other agents. The traf\ufb01c network is modeled"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "by considering an autonomous agent controls in which each"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "intersection decides on duration of green phase. The number"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "of vehicles in different inputs of the intersection are mea-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "sured by the corresponding agent. Any agent interacts with"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "neighbor agents by getting a reward from each decision. This"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "paper proposes that each agent fuzzify the inputs and utilizes"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "in a fuzzy inference system for fuzzy estimation of traf\ufb01c"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "model states. The agent uses a Q-learning approach modi-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "\ufb01ed by Game theory to learn from the past experiences and"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "consider the interaction with neighbor agents. The agent gets"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 15,
                "text": "123"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Arabian Journal for Science and Engineering"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "a reward proportional to its own traf\ufb01c state and a reward from"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "each decision from neighbor agents to update its Q-learning"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "algorithm. The neighbor reward and its weighting in Q-value"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "update is proposed to be fuzzy in the proposed method. The"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "proposed method is applied on a \ufb01ve-intersection traf\ufb01c net-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "work. The simulation results indicate that proposed method"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "outperforms the \ufb01xed time, fuzzy, Q-learning and fuzzy Q-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "learning control methods in the sense of average delay time."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "This paper is unfolds as follows. After this introduction,"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Q-learning and its fuzzy version are described in the next"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "section. Section 3 is devoted to application of Game theory"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "in ITS. Sections 4 and 5 are about problem statement and"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "proposed solution, respectively. Simulation results are given"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "in Sect. 6. Finally, the paper is concluded in Sect. 7."
            }
        ],
        [
            {
                "bold": true,
                "font_size": 12,
                "text": "2 Q-Learning and Fuzzy Q-Learning"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "The objective of agents which act in dynamic environments"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "is making optimum decisions. If the agents are not aware of"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "rewards corresponding to various actions, selecting a proper"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "action would be challenging. To achieve this goal, learning"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "adjusts agents\u2019 action selection based on collected data. Each"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "agent tries to optimize its actions with dynamic environment"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "via trial and error in reinforcement learning (RL). The RL is"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "actually how different situations are mapped upon actions to"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "receive the best results or the highest reward. In many cases,"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "actions in\ufb02uence the reward of next steps as well as affect the"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "reward of its corresponding step. There are model-based [32]"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "as well as model-free [12] RL techniques. In model-free RL,"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the agent does not need explicit modeling of the environ-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ment because its actions could be directly selected based on"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "rewards. Q-learning is a model-independent approach where"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the agent does not access to transfer model [1,31]. Suppose"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "that the agent is in a state s, performs an action a, from"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "which it gets the rewards r from the environment and the"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "environment changes to state s(cid:2). This is given by a tuple in"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the form of (s, a, r , s(cid:2)). State-action value which represents"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the expected total reward resulting from taking action a in"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "state s is denoted by Q-value Q(s, a). The agent starts with"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "random value and after each action they receive a tuple in the"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "form of (s, a, r , s(cid:2)). For each tuple, the value of state-action"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "could be calculated according to the following equation:"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "Q(s, a) = (1 \u2212 \u03b1)Q(s, a)"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "+ \u03b1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "(cid:2)"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "r + \u03b3 max Q(s(cid:2), a(cid:2)) \u2212 Q(s, a)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "(cid:3)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "(1)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "where \u03b1 \u2208 [0, 1] is the learning rate of agent. \u03b1 = 1 means"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "that merely new information is considered and zero means"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "that the agent does not have any learning. \u03b3 \u2208 [0, 1] is dis-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "count factor which determines future rewards. Zero value"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "for this factor makes the agent opportunist which means that"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the agent only considers current reward. On the other hand,"
            }
        ]
    ],
    "page_3": [
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Arabian Journal for Science and Engineering"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "\u03b3 = 1 means that the agent will wait for a longer time to"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "achieve a large reward. Q-learning will converge to optimum"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "value Q\u2217(s, a) with probability of one if all state-action pairs"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "are experienced repetitively and learning rate decrease during"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the time [22]. Generally, RL is useful for solving problems"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "with small dimension discrete state and action space. When"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the dimension of state and action space becomes larger, the"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "size of search table will be so large that it makes the algorithm"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "very slow due to computational time. On the other hand, when"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the states or actions are stated continuously, using search"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "table will not be possible. To tackle this problem, fuzzy the-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ory is employed. If the intelligent agent has a proper fuzzy"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "set as expert knowledge about the desired area, the ambigu-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ity could be resolved. Thus, intelligent agent can understand"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "vague objectives and unknown environment. In practice, the"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "action in large spaces is facilitated by eliminating Q-values"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "table. In this method everything is based on quality values"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "and fuzzy inference. Fuzzy inference system (FIS) deals with"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "input and Q-learning algorithm uses the follower section and"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "its active rules as states. Reward signal of Q-algorithm is built"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "in accordance with fuzzy logic, environment reward signal"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "and performance estimation of current action. It is tried to"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "select the action which maximizes the reward signal [9,14]."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Learning system is able to select one action among j actions"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "for each rule. j-th possible action in i-th rule is denoted by"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "a[i, j] and its value is shown by q[i, j] consider the follow-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ing rules [9]:"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "If x is si"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "then a[i, 1] with q[i, 1]"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "a[i, 2] with q[i, 2]"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "or"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "..."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "or"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "a[i, j] with q[i, j]"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "Learning should \ufb01nd the best result for each rule. If the agent"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "selects an action which results in high value, it may learn"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "optimum policy. Thus, fuzzy inference system may obtain"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "necessary action for each rule [9]."
            }
        ],
        [
            {
                "bold": true,
                "font_size": 12,
                "text": "3 Game Theory in ITS"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "Relation between agent-oriented environments and games"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "theory originates from the fact that each state of agent-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "oriented environments can be resembled to a game environ-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ment. Pro\ufb01t function of players would be current state of the"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "environment and goal of players is to move toward balanced"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "or equilibrium point (reaching the best decision-making pol-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "icy). Some scholars have studied the application of Game"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "theory to control of traf\ufb01c lights [15,16]. They integrate"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Game theory into the multi-agent interaction approach. Some"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "of them suit the traf\ufb01c problem into a rigorous mathemati-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "cal game model [5,8,11], while others modify the learning"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "method of agents based on Game theory [33]. In [5], signal-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ized intersections are modeled as \ufb01nite controlled Markov"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "chains and each intersection is seen as non-cooperative game"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "where each player try to minimize its queue. The solutions"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "are given as Nash equilibrium and Stackelberbg equilibrium"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "and the simulation results indicate shorter queue length than"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "adaptive control. In [8], a two-player non-cooperative game"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "is articulated between user seeking a path to minimize the"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "expected trip cost and choosing link performance scenarios"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "to maximize the expected trip cost. It shows that the Nash"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "equilibrium point measures network performance. Intelli-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "gent traf\ufb01c control is expressed as a Cournot game where the"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "traf\ufb01c authority and the users choose their strategies simulta-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "neously and as a bi-level Stackelberg game where the traf\ufb01c"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "authority is the leader which determines the signal settings in"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "anticipation of the user reactions. In [33], Game theory is used"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "to address coordination between agents based on traf\ufb01c signal"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "control with Q-learning. It speci\ufb01es strategies (C(m) ={red"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "light time plus 4 s, red light time plus 8 s, red light time minus"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "4 s, red light time minus 8 s,unchangeably}) and actions"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "(S(n) ={east west straight and right turn, south north straight"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "and right turn, east west left turn, south north left turn})."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Then, an interaction mathematical model via Game theory"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "as a four parameter group G = {B, A, I , U } is presented."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "B is a group of decision-makers as players. A is a group of"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "any possible strategies and actions, i.e. A = C(m) \u2217 S(n)."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "I represents the information which agents masters. U is the"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "bene\ufb01t function which adopts Q-value. So, the Nash equi-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "librium is [33]:"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Ui (a\u2217"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "i"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": ", a\u2217"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "\u2212i"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": ") \u2265 Ui (ai , a\u2217"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "\u2212i"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": ")"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "(2)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "(3)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "where ai and a\u2212i denote action of i-th agent and actions of"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "other agents, respectively. a\u2217"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "\u2212i represent the actions"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "at Nash equilibrium. The renewed Q-values in distributed"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "reinforcement Q-learning are used to build the payoff values."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Q-value function is updated as:"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "i and a\u2217"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "Qi (si , ai ) = (1 \u2212 \u03b1i )Qi (si , ai )"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "\u23a1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "+ \u03b1i"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "\u23a3ri (si , ai ) +"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "n(cid:6)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "j=1, j(cid:6)=i"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "f (i, j)r j (si , ai )"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "\u23a4"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "\u23a6"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "(4)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "+ \u03b3 max(Qi (s(cid:2)"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "i"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": ", a(cid:2)"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "i"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": ") \u2212 Qi (si , ai ))"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "where \u03b1 and \u03b3 are learning rate and discount factor, respec-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "tively. si and ai are current state of traf\ufb01c environment and"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "current action, respectively. s(cid:2)"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "i is its next state, n is the num-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ber of traf\ufb01c signal control agents surrounding i-th agent,"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Qi (si , ai ) is the Q-value function for i-th agent when selects"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "action ai in state si . ri (si , ai ) is reward function of i-th agent"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "and r j (si , ai ) is reward function of j-th agent neighboring i-"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 15,
                "text": "123"
            }
        ]
    ],
    "page_4": [
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "th agent. f (i, j) \u2208 [0, 1] is a weighted function which shows"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the effect of r j (si , ai ) on i-th agent. Mathematical functions"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "are suggested in [33] for r (s, a) and f (i, j). Assumption of"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "discrete action-state space and determination of reward and"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "weighting functions are drawbacks of that work."
            }
        ],
        [
            {
                "bold": true,
                "font_size": 12,
                "text": "4 Problem Statements"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "Consider a traf\ufb01c network in which the lights of each inter-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "section is controlled by an autonomous agents without any"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "centralized management. Some sensors which are installed"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "below the surface of surrounding streets or traf\ufb01c cameras"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "of each intersection provide information about traf\ufb01c situa-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "tion for the corresponding agent. An agent has to decide on"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "duration of green light at north\u2013south (NS) and west\u2013east"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "(WE) paths. Also, any agent interacts with neighbor agents."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Anyway, the agent is expected to schedule traf\ufb01c lights opti-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "mally, in the sense of average delay, based on the received"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "information from its sensors and received information from"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "neighbor agents."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "The agents may have little knowledge about others\u2019 deci-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "sion due to distribution of information. Even if an agent has"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "previous known information about others\u2019 decision, it is not"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "valid as other agents are also learning. Thus, the environ-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ment is dynamic and the behavior of other agents may change"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "during time. Lack of prediction of other agents causes uncer-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "tainty in problem solving procedure. This paper looks for a"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "decision-making algorithm for lights control agents which"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "considers neighbor agents information in addition to its own"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "information."
            }
        ],
        [
            {
                "bold": true,
                "font_size": 12,
                "text": "5 Proposed Algorithm"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "We consider a constant duration T for green plus red phases."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "So, if the agent determines the green phase duration tg, then"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the red phase duration is tr = T \u2212 tg. Any typical agent i"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "receives number of vehicles on the NS and WE streets from"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "its own sensors and the green phase duration of neighbor"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "agent j in order to schedule its own green phase duration."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "This paper proposes an autonomous agent with structure in"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Fig. 1 to control each intersection."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "The number of vehicles in WE and NS streets which are mea-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "sured by sensors are fuzzi\ufb01ed. Then, a fuzzy inference engine"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "with rules as Eq. 2 are employed to \ufb01re the corresponding"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "output membership functions. Finally, defuzzi\ufb01cation results"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "to duration of green phase in NS path (t N S"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "). Thus, the dura-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "= T \u2212t N S"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "tion of green phase in other path, WE, is t W E"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": ". We"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "propose that, Q-value function which is updated by Eq. 4 be"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the value of each action in Eq. 2 which is denoted by q[i, j]."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "This update equation takes the neighbor agents\u2019 decision into"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "account."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "g"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "g"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "g"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 15,
                "text": "123"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Arabian Journal for Science and Engineering"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Fig. 1 The proposed structure for a typical agent"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "The i-th agent takes decision of neighbor agent"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "j into"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "account by reward r j (si , ai ) and a weighting function"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "f (i, j). The reward is calculated based on average delay"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "obtained from the decision made by the agent and current"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "traf\ufb01c situation in a fuzzy manner. A fuzzy inference engine"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "obtains these two inputs after fuzzi\ufb01cation and gives the"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "reward after defuzzi\ufb01cation; see Fig. 1. weighting function"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "f (i, j) \u2208 [0, 1] shows the effect of r j (si , ai ) on the deci-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "sion of i-th agent. This weight is also calculated by a fuzzy"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "inference engine. This engine takes its own tg, the neighbor"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "agents\u2019 tg, and number of waited vehicles and gives f (i, j)."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Suitable choice for reward and weighting function plays a"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "signi\ufb01cant role in agent learning. The agent with structure in"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Fig. 1 runs the following algorithm:"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "1. Initial value of Qi -value for i-th traf\ufb01c signal control"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "agent is in the form of \u2200(si , ai ) : Qi (si , ai ) = 0."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "2. Observing si by WE and NS sensors which is the current"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "state of i-th intersection."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "3. Selecting a proper estimation for desired state by fuzzy"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "inference system."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "4. Calculating the reward related to i-th and j-th traf\ufb01c"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "signal control agent and the weighting function for neigh-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "boring agents separately."
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "5. Observing new state s(cid:2)"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "i ."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "6. Updating Qi -value according to Eq. 4."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "7. Returning to step 2 till the variation of Q-value becomes"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "less than (cid:4)."
            }
        ],
        [
            {
                "bold": true,
                "font_size": 12,
                "text": "6 Simulation Results"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "Consider a traf\ufb01c network with a center and four neighbor"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "intersection. The delay in each intersection depends on phys-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ical characteristics of the intersection, traf\ufb01c light scheduling"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "and number of cars in input streets. We utilized traf\ufb01c model"
            }
        ]
    ],
    "page_5": [
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Arabian Journal for Science and Engineering"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.8"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.6"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.4"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "VL"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "L"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "M"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "H"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "VH"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "500"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1000 1500 2000 2500 3000 3500"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 9,
                "text": "Number of vehivles"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.8"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.6"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.4"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "VS"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "S"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "M"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "H"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "VH"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "10"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "20"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "30"
            },
            {
                "bold": false,
                "font_size": 9,
                "text": "Delay"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "40"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "50"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "60"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Fig. 2 Membership function of number of vehicles enter the street for"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "reward FIS"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Fig. 3 Membership function of average delay for reward FIS"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "which is given by the American Highway Capacity Manual"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "(HCM) [4, Eq.20]:"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "d = 0.38"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "C(1 \u2212 \u03bb)2"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "1 \u2212 x"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "(cid:9)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "+ 173x 2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "(x \u2212 1) +"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "(cid:11)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "(cid:10)"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "(x \u2212 1)2 + 16x"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "C"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "(5)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "where d, C, \u03bb, and x are average delay (s), cycle time (s),"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "green ratio, and degree of saturation, respectively. \u03bb = g"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "c"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "and x = v"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "c , where c, g, and v are capacity (vehicle per hour),"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "green time (sec), and input volume, respectively. We use this"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "model to calculate average delay based on the green phase"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "duration and number of vehicles. For more details of this"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "equation, we refer to [4]."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "Assume that C = T = 100 s and c = 3500 veh/h. v is"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "volume of vehicles entering each street which varies between"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "0 to 3500 veh/h. g is duration of the green phase which each"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "agent selects considering fuzzy Q-learning and interaction"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "with adjacent agents. The traf\ufb01c network simulation algo-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "rithm is as follow:"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "1. The volume of vehicles entering each intersection (v) are"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "randomly generated by a discrete uniform distribution on"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the interval [0, 3500]."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "2. Average delay is calculated by Eq. 5."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "3. Each agent decides on the time of green phase g."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "4. Go to step 1 until end of simulation time."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "Assume structure of the agents as in Fig. 1 with the Mam-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "dani FIS with input membership function as in Fig. 2 for"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "number of input vehicles and Fig. 3 for average delay to"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "calculate the reward functions r j (si , ai ). Centroid defuzzi-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "\ufb01cation by the output membership function as in Fig. 4 is"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "considered to estimate a reward value in interval [\u2212 3, 3]."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "The weighting function FIS has number of vehicles, its own"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "green phase duration and the neighbor agents\u2019 green phase"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "duration as inputs. Figure 2 shows the membership function"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "for number of vehicles, and Fig. 5 depicts the membership"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "VS"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.8"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.6"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.4"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "-3"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "S"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "M"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "H"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "VH"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "-2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "-1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            },
            {
                "bold": false,
                "font_size": 9,
                "text": "Reward"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "3"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Fig. 4 Membership function of output for reward FIS"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.8"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.6"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.4"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "L"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "M"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "H"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "20"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "40"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "60"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "80"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "100"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 9,
                "text": "Green phase duration"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Fig. 5 Membership function of green phase duration for weighting"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "function FIS"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "function for its own and neighbor green phase duration. Cen-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "troid defuzzi\ufb01cation is applied to calculate weights on output"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "membership function as in Fig. 6 which should be a value"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "between 0 and 1."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Finally, the agent uses fuzzy Q-learning (Eq. 2) with Q-value"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "update rule (Eq. 4) where learning and discount factor are"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "selected to be 0.5 and 0.7, respectively. The membership"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "function for each measured number of vehicles is shown in"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "Fig. 7. The output estimates green phase duration with mem-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "bership functions as in Fig. 8."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "The proposed method is compared with Fuzzy Q-learning"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "(using Eq. 2 where q[i, j] is the Q-value which updates"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "with Eq. 1), Q-learning (using Q-learning method with Q-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "value which updates with Eq. 1), fuzzy(using traditional"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "fuzzy inference method) and \ufb01xed time (tg = 60 s) in"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "the sense of total average delay. Average delay in each"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 15,
                "text": "123"
            }
        ]
    ],
    "page_6": [
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "VS"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "S"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "M"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "H"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "VH"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Arabian Journal for Science and Engineering"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.4"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.6"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.8"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 9,
                "text": "Weight"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.8"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.6"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.4"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Fig. 6 Membership function of output for weighting function FIS"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.8"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.6"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.4"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "VL"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "L"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "H"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "VH"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "500"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1000 1500 2000 2500 3000 3500"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 9,
                "text": "Number of vehivles"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Fig. 9 Delay of the proposed method, \ufb01xed time, fuzzy Q-learning,"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Q-learning and fuzzy in each time step"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Fig. 7 Membership function of number of vehicles for fuzzy Q-learning"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.8"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.6"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.4"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0.2"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "0"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "S1"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "S2 S3 S4 S5 S6 S7 S8 S9"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "20"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "40"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "60"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "80"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "100"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 9,
                "text": "green phase duration"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Fig. 10 Average of delay for the proposed method, \ufb01xed time, fuzzy,"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Q-learning, fuzzy Q-learning"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Fig. 8 Membership function of green phase duration for fuzzy Q-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "learning"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "time interval is depicted in Fig. 9, and the total average"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "delay is illustrated in Fig. 10. The results illustrate that"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "total average delay decrease from more than 50 s for \ufb01xed"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "time scheduling to approximately 15 s for the proposed"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "method."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "The reward received from the neighbor and weighted func-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "tions of neighboring agents are factors learning algorithm."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "These parameters are fuzzi\ufb01ed through a FIS. Also, the"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "number of vehicles in each street is measured and fuzzi-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "\ufb01ed to be used in decision-making process. The simulation"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "results were compared with \ufb01xed time method and other"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "intelligent methods. The results revealed that our proposed"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "method achieves considerable reduction of average delay in"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "intersections."
            }
        ],
        [
            {
                "bold": true,
                "font_size": 12,
                "text": "7 Conclusion"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 10,
                "text": "In this study, an intelligent control method of a controlling"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "traf\ufb01c network was performed to decrease average delay"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "time. Each traf\ufb01c light is considered as a learning agent."
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "This paper proposed a structure for the agents. Each agent"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "learn to decide on the duration of green phase through a"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "fuzzy Q-learning algorithm which is modi\ufb01ed by Game the-"
            },
            {
                "bold": false,
                "font_size": 10,
                "text": "ory. Each agent receives a reward from neighbor agents."
            }
        ],
        [
            {
                "bold": true,
                "font_size": 12,
                "text": "References"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "1. Abdoos, M.; Mozayani, N.; Bazzan, A.L.: Traf\ufb01c light control in"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "non-stationary environments based on multi agent q-learning. In:"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "14th International IEEE Conference on Intelligent Transportation"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Systems (ITSC), pp. 1580\u20131585. IEEE (2011)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "2. Abdulhai, B.; Pringle, R.; Karakoulas, G.J.: Reinforcement learn-"
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "ing for true adaptive traf\ufb01c signal control. J. Transp. Eng. 129(3),"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "278\u2013285 (2003)"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 15,
                "text": "123"
            }
        ]
    ],
    "page_7": [
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Arabian Journal for Science and Engineering"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "3. Adler, J.L.; Satapathy, G.; Manikonda, V.; Bowles, B.; Blue, V.J.: A"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "multi-agent approach to cooperative traf\ufb01c management and route"
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "guidance. Transp. Res. Part B Methodol. 39(4), 297\u2013318 (2005)"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "4. Akgungor, A.P.; Bullen, A.G.R.: Analytical delay models for sig-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "nalized intersections. In: 69th ITE Annual Meeting, Nevada, USA"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "(1999)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "5. Alvarez, I.; Poznyak, A.; Malo, A.: Urban traf\ufb01c control problem a"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "game theory approach. In: 47th IEEE Conference on Decision and"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Control, pp. 2168\u20132172. IEEE (2008)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "6. Balaji, P.; German, X.; Srinivasan, D.: Urban traf\ufb01c signal control"
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "using reinforcement learning agents. IET Intell. Transp. Syst. 4(3),"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "177\u2013188 (2010)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "7. Bazzan, A.L.; Klgl, F.: A review on agent-based technology for traf-"
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "\ufb01c and transportation. Knowl. Eng. Rev. 29(03), 375\u2013403 (2014)"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "8. Bell, M.G.: A game theory approach to measuring the performance"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "reliability of transport networks. Transp. Res. Part B Methodol."
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "34(6), 533\u2013545 (2000)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "9. Bonarini, A.; Lazaric, A.; Montrone, F.; Restelli, M.: Reinforce-"
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "ment distribution in fuzzy q-learning. Fuzzy Sets Syst. 160(10),"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "1420\u20131443 (2009)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "10. Bull, L.; ShaAban, J.; Tomlinson, A.; Addison, J.D.; Heydecker,"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "B.G.: Towards distributed adaptive control for road traf\ufb01c junction"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "signals using learning classi\ufb01er systems. In: Bull, L. (ed.) Applica-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "tions of Learning Classi\ufb01er Systems, pp. 276\u2013299. Springer, Berlin"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "(2004)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "11. Chen, O.; Ben-Akiva, M.: Game-theoretic formulations of interac-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "tion between dynamic traf\ufb01c control and dynamic traf\ufb01c assign-"
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "ment. Transp. Res. Rec. J. Transp. Res. Board 1617, 179\u2013188"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "(1998)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "12. Chin, Y.K.; Bolong, N.; Kiring, A.; Yang, S.S.; Teo, K.T.K.: Q-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "learning based traf\ufb01c optimization in management of signal timing"
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "plan. Int. J. Simul. Syst. Sci. Technol. 12(3), 29\u201335 (2011)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "13. Da Silva, B.C.; Basso, E.W.; Perotto, F.S.; C Bazzan, A.L.; Engel,"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "P.M.: Improving reinforcement learning with context detection."
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "In: Proceedings of the Fifth International Joint Conference on"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Autonomous Agents and Multiagent Systems, pp. 810\u2013812. ACM"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "(2006)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "14. Glowaty, G.: Enhancements of fuzzy q-learning algorithm. Com-"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 8,
                "text": "put. Sci. 7, 77\u201387 (2005)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "15. Goyal, T.; Kaushal, S.: An intelligent scheduling scheme for real-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "time traf\ufb01c management using cooperative game theory and ahp-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "topsis methods for next generation telecommunication networks."
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "Expert Syst. Appl. 86, 125\u2013134 (2017)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "16. Groot, N.; Zaccour, G.; De Schutter, B.: Hierarchical game the-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "ory for system-optimal control: applications of reverse stackelberg"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "games in regulating marketing channels and traf\ufb01c routing. IEEE"
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "Control Syst. 37(2), 129\u2013152 (2017)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "17. Houli, D.; Zhiheng, L.; Yi, Z.: Multiobjective reinforcement learn-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "ing for traf\ufb01c signal control using vehicular ad hoc network."
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "EURASIP J. Adv. Signal Process. 1, 724,035 (2010)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "18. Iyer, V.; Jadhav, R.; Mavchi, U.; Abraham, J.: Intelligent traf\ufb01c"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "signal synchronization using fuzzy logic and q-learning. In: Inter-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "national Conference on Computing, Analytics and Security Trends"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "(CAST), pp. 156\u2013161. IEEE (2016)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "19. Kponyo, J.; Nwizege, K.; Opare, K.; Ahmed, A.; Hamdoun, H.;"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Akazua, L.; Alshehri, S.; Frank, H.: A distributed intelligent traf-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "\ufb01c system using ant colony optimization: a netlogo modeling"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "approach. In: International Conference on Systems Informatics,"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Modelling and Simulation (SIMS), pp. 11\u201317. IEEE (2016)"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "20. Liu, Z.: A survey of intelligence methods in urban traf\ufb01c signal"
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "control. IJCSNS Int. J. Comput. Sci. Netw. Secur. 7(7), 105\u2013112"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "(2007)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "21. Medina, J.C.; Hajbabaie, A.; Benekohal, R.F.: Arterial traf\ufb01c con-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "trol using reinforcement learning agents and information from"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "adjacent intersections in the state and reward structure. In: 2010"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "13th International IEEE Conference on Intelligent Transportation"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Systems (ITSC), pp. 525\u2013530. IEEE (2010)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "22. Pacheco, J.C.; Rossetti, R.J.: Agent-based traf\ufb01c control: a fuzzy"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "q-learning approach. In: 13th International IEEE Conference on"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Intelligent Transportation Systems (ITSC), pp. 1172\u20131177. IEEE"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "(2010)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "23. Prashanth, L.; Bhatnagar, S.: Reinforcement learning with function"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "approximation for traf\ufb01c signal control. IEEE Trans. Intell. Transp."
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "Syst. 12(2), 412\u2013421 (2011)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "24. Rida, M.: Modeling and optimization of decision-making process"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "during loading and unloading operations at container port. Arab. J."
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "Sci. Eng. 39(11), 8395\u20138408 (2014)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "25. Roess, R.P.; Prassas, E.S.; McShane, W.R.: Traf\ufb01c Engineering."
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "Prentice Hall, Englewood Cliffs (2004)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "26. Salkham, A.; Cunningham, R.; Garg, A.; Cahill, V.: A collaborative"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "reinforcement learning approach to urban traf\ufb01c control optimiza-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "tion. In: Proceedings of IEEE/WIC/ACM International Conference"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "on Web Intelligence and Intelligent Agent Technology, pp. 560\u2013"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "566. IEEE Computer Society (2008)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "27. Schaefer, M.; Vok\u02c7r\u00ednek, J.; Pinotti, D.; Tango, F.: Multi-agent traf\ufb01c"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "simulation for development and validation of autonomic car-to-car"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "systems. In: McCluskey, Th.L., Kotsialos, A., M\u00fcller, J.P., Kl\u00fcgl, F.,"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Rana, O., Schumann, R. (eds.) Autonomic Road Transport Support"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Systems, pp. 165\u2013180. Springer, Berlin (2016)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "28. Steingrover, M.; Schouten, R.; Peelen, S.; Nijhuis, E.; Bakker, B.:"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "Reinforcement learning of traf\ufb01c light controllers adapting to traf\ufb01c"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "congestion. In: BNAIC, pp. 216\u2013223. Citeseer (2005)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "29. Teknomo, K.: Application of microscopic pedestrian simulation"
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "model. Transp. Res. Part F Traf\ufb01c Psychol. Behav. 9(1), 15\u201327"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "(2006)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "30. Vilarinho, C.; Tavares, J.P.; Rossetti, R.J.: Intelligent traf\ufb01c lights:"
            },
            {
                "bold": true,
                "font_size": 8,
                "text": "green time period negotiation. Transp. Res. Procedia 22, 325\u2013334"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "(2017)"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 8,
                "text": "31. Watkins, C.J.; Dayan, P.: Q-learning. Mach. Learn. 8(3\u20134), 279\u2013"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "292 (1992)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "32. Wiering, M.: Multi-agent reinforcement learning for traf\ufb01c light"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "control. In: ICML, pp. 1151\u20131158 (2000)"
            }
        ],
        [
            {
                "bold": false,
                "font_size": 8,
                "text": "33. Xinhai, X.; Lunhui, X.: Traf\ufb01c signal control agent interaction"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "model based on game theory and reinforcement learning. In:"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "International Forum on Computer Science-Technology and Appli-"
            },
            {
                "bold": false,
                "font_size": 8,
                "text": "cations, vol. 1, pp. 164\u2013168. IEEE (2009)"
            }
        ],
        [
            {
                "bold": true,
                "font_size": 15,
                "text": "123"
            }
        ]
    ]
}